{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2020-10-21 05:33:01.018827: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Create YOLOv3 model with 9 anchors and 1 classes.\n",
      "Load weights model_data/yolo_train_weights_2.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "Train on 2167 samples, val on 67 samples, with batch size 64.\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 608s 18s/step - loss: 6792.8742 - val_loss: nan\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 605s 18s/step - loss: 6262.3196 - val_loss: nan\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 608s 18s/step - loss: 5780.4093 - val_loss: nan\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 602s 18s/step - loss: 5341.4219 - val_loss: nan\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 605s 18s/step - loss: 4941.9622 - val_loss: nan\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 606s 18s/step - loss: 4578.2628 - val_loss: nan\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 601s 18s/step - loss: 4247.5683 - val_loss: nan\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 600s 18s/step - loss: 3946.8066 - val_loss: nan\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 619s 19s/step - loss: 3672.9853 - val_loss: nan\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 631s 19s/step - loss: 3423.5317 - val_loss: nan\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 639s 19s/step - loss: 3196.2294 - val_loss: nan\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 640s 19s/step - loss: 2988.7104 - val_loss: nan\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 638s 19s/step - loss: 2799.3160 - val_loss: nan\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 641s 19s/step - loss: 2625.5791 - val_loss: nan\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 637s 19s/step - loss: 2466.7165 - val_loss: nan\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 641s 19s/step - loss: 2320.7976 - val_loss: nan\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 640s 19s/step - loss: 2186.5528 - val_loss: nan\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 643s 19s/step - loss: 2063.1975 - val_loss: nan\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 641s 19s/step - loss: 1949.2693 - val_loss: nan\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 641s 19s/step - loss: 1844.2172 - val_loss: nan\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 642s 19s/step - loss: 1747.2786 - val_loss: nan\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 639s 19s/step - loss: 1657.2241 - val_loss: nan\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 610s 18s/step - loss: 1573.6896 - val_loss: nan\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 1496.1924 - val_loss: nan\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 1423.9682 - val_loss: nan\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 599s 18s/step - loss: 1356.8822 - val_loss: nan\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 598s 18s/step - loss: 1294.1003 - val_loss: nan\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 598s 18s/step - loss: 1235.6174 - val_loss: nan\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 1180.8478 - val_loss: nan\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 598s 18s/step - loss: 1129.5432 - val_loss: nan\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 1081.5357 - val_loss: nan\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 598s 18s/step - loss: 1036.1885 - val_loss: nan\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 993.7721 - val_loss: nan\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 596s 18s/step - loss: 953.9276 - val_loss: nan\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 916.1568 - val_loss: nan\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 595s 18s/step - loss: 880.7818 - val_loss: nan\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 847.3062 - val_loss: nan\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 815.5859 - val_loss: nan\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 596s 18s/step - loss: 785.6039 - val_loss: nan\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 757.2105 - val_loss: nan\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 730.3018 - val_loss: nan\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 704.9254 - val_loss: nan\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 596s 18s/step - loss: 680.7852 - val_loss: nan\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 596s 18s/step - loss: 657.6871 - val_loss: nan\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 635.7538 - val_loss: nan\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 614.9274 - val_loss: nan\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 595.0960 - val_loss: nan\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 576.2548 - val_loss: nan\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 597s 18s/step - loss: 558.1317 - val_loss: nan\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 596s 18s/step - loss: 540.9486 - val_loss: nan\n",
      "Unfreeze all of the layers.\n",
      "Train on 2167 samples, val on 67 samples, with batch size 64.\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 3668s 111s/step - loss: 402.3599 - val_loss: 230.9874\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 3644s 110s/step - loss: 236.7216 - val_loss: 215.6439\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 3646s 110s/step - loss: 169.7115 - val_loss: 161.1117\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 3650s 111s/step - loss: 134.4621 - val_loss: 111.6518\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 3649s 111s/step - loss: 111.5799 - val_loss: 83.9895\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 3651s 111s/step - loss: 95.1867 - val_loss: 72.4070\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 3648s 111s/step - loss: 82.9113 - val_loss: 69.4503\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 3654s 111s/step - loss: 73.6545 - val_loss: 64.9885\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 3648s 111s/step - loss: 66.7190 - val_loss: 61.7886\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 3648s 111s/step - loss: 61.1842 - val_loss: 58.8968\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 3653s 111s/step - loss: 56.5252 - val_loss: 55.2228\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 3661s 111s/step - loss: 52.2407 - val_loss: 51.3928\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 3660s 111s/step - loss: 49.2701 - val_loss: 48.9394\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 3663s 111s/step - loss: 46.1767 - val_loss: 46.3227\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 3662s 111s/step - loss: 43.6418 - val_loss: 44.6785\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 3665s 111s/step - loss: 41.6171 - val_loss: 41.0136\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 3667s 111s/step - loss: 39.6723 - val_loss: 39.9525\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 3662s 111s/step - loss: 38.1846 - val_loss: 38.5968\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 3760s 114s/step - loss: 36.5538 - val_loss: 36.7885\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 3807s 115s/step - loss: 35.4196 - val_loss: 35.8624\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 3771s 114s/step - loss: 34.3952 - val_loss: 34.6402\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 3668s 111s/step - loss: 33.4702 - val_loss: 34.0576\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 3831s 116s/step - loss: 32.3663 - val_loss: 34.0083\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 3841s 116s/step - loss: 31.7901 - val_loss: 33.9221\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 3840s 116s/step - loss: 30.8853 - val_loss: 33.2496\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 3840s 116s/step - loss: 30.3148 - val_loss: 32.7804\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 3838s 116s/step - loss: 29.6060 - val_loss: 31.6113\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 3837s 116s/step - loss: 28.9775 - val_loss: 30.8519\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 3831s 116s/step - loss: 28.5190 - val_loss: 30.5817\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 3838s 116s/step - loss: 27.8506 - val_loss: 30.0013\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 3839s 116s/step - loss: 27.4745 - val_loss: 29.8604\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 3829s 116s/step - loss: 26.9334 - val_loss: 28.8444\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 3831s 116s/step - loss: 26.4075 - val_loss: 27.8604\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 3812s 116s/step - loss: 25.9845 - val_loss: 28.5914\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 3699s 112s/step - loss: 25.5898 - val_loss: 27.1461\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 3664s 111s/step - loss: 25.2914 - val_loss: 27.7546\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 3679s 111s/step - loss: 24.8967 - val_loss: 27.7975\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 3677s 111s/step - loss: 24.5980 - val_loss: 27.4494\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 3673s 111s/step - loss: 24.3825 - val_loss: 26.7263\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 4032s 122s/step - loss: 24.0118 - val_loss: 26.7347\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 4986s 151s/step - loss: 23.7169 - val_loss: 26.5692\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 4059s 123s/step - loss: 23.5656 - val_loss: 26.6954\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 3702s 112s/step - loss: 23.2323 - val_loss: 25.6972\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 3697s 112s/step - loss: 22.9529 - val_loss: 26.3825\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 3695s 112s/step - loss: 22.7711 - val_loss: 25.2671\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 3700s 112s/step - loss: 22.4193 - val_loss: 25.0594\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 3705s 112s/step - loss: 22.3690 - val_loss: 24.9425\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 3696s 112s/step - loss: 22.1014 - val_loss: 24.6576\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 3693s 112s/step - loss: 22.1002 - val_loss: 24.3253\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 3683s 112s/step - loss: 21.8340 - val_loss: 25.0840\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
